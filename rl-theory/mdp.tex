\input{settings} % add packages, settings, and declarations in settings.tex

\usepackage[utf8]{inputenc}



\title{note}
\author{Dongkun Zhang}
\date{Jan 2022}

\begin{document}

\maketitle

\section{Markov Decision Process}



\begin{tcolorbox}
\textbf{
    (Definition) Return
} 
\textit{
    Something
}
\begin{align*}
    g_{t_0} = \sum_{t=t_0}^{\infty}\gamma^{t-t_0}r(s_t,a_t)
\end{align*}
\end{tcolorbox}


\begin{tcolorbox}
\textbf{
    (Definition) Index and Goal
} 
\textit{
    the agentâ€™s goal is to obtain a policy which maximises the cumulative discounted reward from $t=0$: 
}
\begin{align*}
    & J(\pi, \gamma, p_0, p) = \mathbb{E}[ g_0 ;\pi, \gamma, p_0, p] \\
    & \max_{\pi} J(\pi, \gamma, p_0, p)
\end{align*}
\end{tcolorbox}




\begin{tcolorbox}
\textbf{
    (Proposition)
} 
\textit{
    Define $ {\rm{Pr}}(s\to s',k,\pi) $ as the probability of transitioning from state $s$ to state $s'$ in $k$ steps under policy $\pi$
}
\begin{align*}
    & J(\pi, \gamma, p_0, p) = J(\pi, \rho^{\pi}) \\
    &= \mathbb{E}_{s \sim \rho^{\pi}(\cdot), \ a \sim \pi(\cdot|s)}[r(s,a)] \\
    &= \sum_{s} \rho^{\pi}(s) \sum_{a} \pi(a|s) r(s,a)
\end{align*}

where $ \rho^{\pi}(s) = \sum_{s_0} p_0(s_0) \sum_{t=0}^{\infty} \gamma^{t} {\rm{Pr}}(s_0 \to s,t,\pi) $ is the (improper) discounted state distribution.

\end{tcolorbox}


\begin{solution}\ \\
Some definitions:
\begin{align*}
    \tau = (s_0,a_0,s_1,a_1,...)
\end{align*}
\begin{align*}
    J(\pi, \gamma, p_0, p) &= \mathbb{E}[ g_0 ;\pi, \gamma, p_0, p] \\
    &= \mathbb{E}[ \sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t) ;\pi, p_0, p] \\
    &= \mathbb{E}_{\tau \sim p_{\tau}(\cdot)}[ \sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t) ] \\
    &= \sum_{\tau} p_{\tau}(\tau) (\sum_{t=0}^{\infty}\gamma^{t}r(s_t,a_t)) \\
    &= \sum_{s_0} p_0(s_0) \sum_{t=0}^{\infty}\gamma^{t} \sum_s\sum_a \pi(a|s) {\rm{Pr}}(s_0\to s,t,\pi) r(s,a) \\
    &= \sum_{s} \rho^{\pi}(s) \sum_{a} \pi(a|s) r(s,a) \\
    &= \mathbb{E}_{s \sim \rho^{\pi}(\cdot), \ a \sim \pi(\cdot|s)}[r(s,a)]
\end{align*}
After.
\end{solution}






\begin{align*}
    V^{\pi}(s) &= \mathbb{E}[g_t | s_t=s; \pi, \gamma, p] \\
    Q^{\pi}(s,a) &= \mathbb{E}[g_t | s_t=s, a_t=a; \pi, \gamma, p]
\end{align*}



\section{Value Function}

 
\begin{align*}
    V^{\pi}(s) &= \mathbb{E}_{a \sim \pi(\cdot|s)}[Q^{\pi}(s,a)] \\
    Q^{\pi}(s,a)&= r(s,a) + \mathbb{E}_{s' \sim p(\cdot|s,a)}[V^{\pi}(s')]
\end{align*}




\begin{align*}
    V(s) &= V^{*}(s) = \max_{\pi}V^{\pi}(s) \\
    Q(s,a) &= Q^{*}(s,a) = \max_{\pi}Q^{\pi}(s,a)
\end{align*}



\begin{align*}
    V(s) = \max_{a}Q^{\pi}(s,a)
\end{align*}




\begin{tcolorbox}
\textbf{
    Value Function
} 
\textit{
    Something
}
\begin{align*}
    Q(s,a) & = r(s,a) + \mathbb{E}_{s' \sim p(\cdot|s,a)}[V(s')] \\
    & = r(s,a) + \mathbb{E}_{s' \sim p(\cdot|s,a)}[\max_{a'}Q(s',a')]
\end{align*}
\end{tcolorbox}

\begin{solution}\ \\
Before
\begin{align*}
    Q(s,a) & = \max_{\pi}Q^{\pi}(s,a) \\
    & = r(s,a) + \max_{\pi}\mathbb{E}_{s' \sim p(\cdot|s,a)}[V(s')]
\end{align*}
After.
\end{solution}





\begin{tcolorbox}
\textbf{
    (Definition) Advantage Function
} 
\textit{
    Something
}
\begin{align*}
    A^{\pi}(s,a) = Q^{\pi}(s,a) - V^{\pi}(s) \\
    \mathbb{E}_{a \sim \pi(\cdot|s)}[A^{\pi}(s,a)] = 0
\end{align*}
\end{tcolorbox}





\begin{tcolorbox}
\textbf{
    Optimal Advantage Function
} 
\textit{
    Something
}
\begin{align*}
    A(s,a) = Q(s,a) - V(s) \\
    A(s,a^{*}) = 0, \quad a^{*} = \mathop{\arg\max}_{a}Q(s,a)
\end{align*}
\end{tcolorbox}

\begin{solution}\ \\
Before
\begin{align*}
    V^{\pi}(s) & = \mathbb{E}_{a \sim \pi(\cdot|s)}[Q^{\pi}(s,a)] \\
    & = \mathbb{E}_{a \sim \pi(\cdot|s)}[A^{\pi}(s,a) + V^{\pi}(s)] \\
    & = \mathbb{E}_{a \sim \pi(\cdot|s)}[A^{\pi}(s,a)] + V^{\pi}(s)
\end{align*}
After.
\end{solution}










\end{document}
